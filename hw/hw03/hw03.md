# Homework03

*Name: 李耀伟*

*Student ID: 2201111734*

- 目标函数: $f=\|max\left(XW,0\right)-Y\|^2_F$，手动写出以下表达式，并用Pytorch验证：

$$
\frac {\partial f}{\partial W},\frac {\partial f}{\partial X},\frac {\partial f}{\partial Y}
$$

- $\frac {\partial f}{\partial W}$
    - 设  $f=\sum_{ij}L^2,L=A-Y,A = max(Z,0), Z=XW$，其中：
    
    $$
    L \in \mathbb R^{m \times n},A\in \mathbb R^{m\times n},Y \in \mathbb R^{m \times n},Z\in \mathbb R^{m\times n},X\in \mathbb R^{m\times k},W\in \mathbb R^{k\times n}
    $$
    
    - $\nabla L=\frac {\partial f}{\partial L} = 2L=2(A-Y),\nabla L \in \mathbb R^{m\times n}$
    <br>
    - $\nabla A=\nabla L\times \frac {\partial L}{\partial A} = 2L \times 1=2(A-Y),\nabla A \in \mathbb R^{m\times n}$
    <br/>
    - $\nabla Z = \nabla A \left( \frac {\partial A}{\partial Z}\right)=\nabla A \cdot I,(if\  Z_{ij}>0\rightarrow I_{ij}=1,else\ I_{ij}=0),\nabla Z\in \mathbb R^{m \times n}$
    <br/>
    - $\nabla W= \nabla Z \frac {\partial Z}{\partial W}=X^T(\nabla A \cdot I)=2X^T \left(\left(A-Y\right) \cdot I\right),\nabla W \in \mathbb R^{k\times n}$， $[\cdot]$ is element-wise product
    <br/>
    - ***代码验证：***
    
    ![Untitled](autograd%20b9e2eb8eb56d49a79e2634258c85d6eb/Untitled.png)
    <br/>
- $\frac {\partial f}{\partial X}$
    - 续前节，$\nabla X= \nabla Z \frac {\partial Z}{\partial X}=(\nabla A \cdot I) W^T=2 \left(\left(A-Y\right) \cdot I\right)w^T,\nabla X \in \mathbb R^{m\times k}$,  $[\cdot]$ is element-wise product<br/>
    - ***代码验证：***
    
    ![Untitled](autograd%20b9e2eb8eb56d49a79e2634258c85d6eb/Untitled%201.png)

    <br/>
- $\frac {\partial f}{\partial Y}$
    - 续前节，$\nabla Y= \nabla L \frac {\partial L}{\partial Y}=\nabla L \times -1=-2(A-Y),\nabla Y \in \mathbb R^{m\times n}$ <br/>
    - ***代码验证：***
    
    ![Untitled](autograd%20b9e2eb8eb56d49a79e2634258c85d6eb/Untitled%202.png)
    

```python
import torch

torch.manual_seed(0)

x = torch.randn(10, 4, requires_grad=True)
W = torch.randn(4, 4, requires_grad=True)
y = torch.randn(10, 4, requires_grad=True)

Z = x @ W 
m = (x @ W) >= 0

A = Z * m

L = A - y

f = (L**2).sum()

f.backward()
print("="*80)
print("###grad_W autograd: \n", W.grad, "\n")
print("###verify W_grad: 2 * x.T @ ((A-y) * (Z >= 0))  \n", 2 * x.T @ ((A-y) * (Z >= 0)))
print("="*80)
print("###grad_X autograd: \n", x.grad, "\n")
print("###verify X_grad: 2 * ((A-y) * (Z >= 0))) @ W.T  \n", 2 * ((A-y) * (Z >= 0)) @ W.T)
print("="*80)
print("###grad_Y autograd: \n", y.grad, "\n")
print("###verify W_grad: -2 * (A-y)  \n", -2 * (A-y))
```

```python
output: 
================================================================================
###grad_W autograd: 
 tensor([[ 18.2980,   2.7573,   2.3914,  -0.1974],
        [ 11.0817,   6.6428,   2.5163, -20.3225],
        [ -8.6662,   3.4506,  -1.8979,  -3.3608],
        [-21.1681,  -6.6739,  -1.0693,  27.0278]])

###verify W_grad: 2 * x.T @ ((A-y) * (Z >= 0))
 tensor([[ 18.2980,   2.7573,   2.3914,  -0.1974],
        [ 11.0817,   6.6428,   2.5163, -20.3225],
        [ -8.6662,   3.4506,  -1.8979,  -3.3608],
        [-21.1681,  -6.6739,  -1.0693,  27.0278]], grad_fn=<MmBackward0>)
================================================================================
###grad_X autograd:
 tensor([[  1.1002,   0.0860,   5.3377,   0.2788],
        [  0.9583,  10.4633, -13.5234, -16.3639],
        [ -0.8712,  -0.9272,  -0.7764,   2.0790],
        [ -1.4504,   5.6914,   0.7613,  -0.9693],
        [ -1.2892,  -3.4714,  -1.9788,   4.8091],
        [ -4.0523,  -4.3127,  -3.6114,   9.6703],
        [ -0.7312,  -0.7782,  -0.6516,   1.7449],
        [ -0.8191,  -0.8718,  -0.7300,   1.9547],
        [  1.0350,   2.9930,  -6.6743,  -7.5333],
        [ -2.4616,  -2.4243,  -2.1164,   5.7128]])

###verify X_grad: 2 * ((A-y) * (Z >= 0))) @ W.T
 tensor([[  1.1002,   0.0860,   5.3377,   0.2788],
        [  0.9583,  10.4633, -13.5234, -16.3639],
        [ -0.8712,  -0.9272,  -0.7764,   2.0790],
        [ -1.4504,   5.6914,   0.7613,  -0.9693],
        [ -1.2892,  -3.4714,  -1.9788,   4.8091],
        [ -4.0523,  -4.3127,  -3.6114,   9.6703],
        [ -0.7312,  -0.7782,  -0.6516,   1.7449],
        [ -0.8191,  -0.8718,  -0.7300,   1.9547],
        [  1.0350,   2.9930,  -6.6743,  -7.5333],
        [ -2.4616,  -2.4243,  -2.1164,   5.7128]], grad_fn=<MmBackward0>)
================================================================================
###grad_Y autograd:
 tensor([[ 2.8885e+00,  4.1639e+00,  3.4134e+00,  3.0501e+00],
        [-1.0589e+01, -2.7045e+00, -2.1849e+00, -1.7039e-01],
        [ 6.5523e-01, -1.5214e+00, -3.1982e+00, -1.5687e+00],
        [-1.5009e+00, -3.8551e+00,  4.9843e-01,  1.2764e+00],
        [-6.6077e-03, -1.0689e+00,  1.8791e+00, -4.2604e+00],
        [ 3.8829e+00,  1.5830e+00, -4.0504e-02, -7.2968e+00],
        [-4.3767e-01, -4.8701e+00, -1.4583e-01, -1.3166e+00],
        [ 1.9250e+00,  6.9834e-01, -1.8429e+00, -1.4750e+00],
        [-5.0359e+00, -9.2744e-01,  3.8436e+00, -8.0509e-01],
        [ 2.4780e-01,  2.3296e+00, -1.7491e-01, -4.2519e+00]])

###verify W_grad: -2 * (A-y)
 tensor([[ 2.8885e+00,  4.1639e+00,  3.4134e+00,  3.0501e+00],
        [-1.0589e+01, -2.7045e+00, -2.1849e+00, -1.7039e-01],
        [ 6.5523e-01, -1.5214e+00, -3.1982e+00, -1.5687e+00],
        [-1.5009e+00, -3.8551e+00,  4.9843e-01,  1.2764e+00],
        [-6.6077e-03, -1.0689e+00,  1.8791e+00, -4.2604e+00],
        [ 3.8829e+00,  1.5830e+00, -4.0504e-02, -7.2968e+00],
        [-4.3767e-01, -4.8701e+00, -1.4583e-01, -1.3166e+00],
        [ 1.9250e+00,  6.9834e-01, -1.8429e+00, -1.4750e+00],
        [-5.0359e+00, -9.2744e-01,  3.8436e+00, -8.0509e-01],
        [ 2.4780e-01,  2.3296e+00, -1.7491e-01, -4.2519e+00]],
       grad_fn=<MulBackward0>)
```
